{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from helpers import load_train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = load_train_data(\"./dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split them into training data & validation data for this ablation study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln = len(x_train)  # or len(y_train) since they are the same\n",
    "\n",
    "x_train, x_val = x_train[: int(ln * 0.9)], x_train[int(ln * 0.9) :]\n",
    "y_train, y_val = y_train[: int(ln * 0.9)], y_train[int(ln * 0.9) :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_exp(x):\n",
    "    return np.exp(np.clip(x, -100, 100))\n",
    "\n",
    "\n",
    "def logistic_regression(y, tx, initial_w, max_iters, gamma, weight):\n",
    "    \"\"\"Logistic regression using gradient descent. (y in {-1, 1})\"\"\"\n",
    "\n",
    "    def stable_sigmoid(t):\n",
    "        \"\"\"Numerically stable sigmoid\"\"\"\n",
    "        return np.where(t >= 0, 1 / (1 + safe_exp(-t)), safe_exp(t) / (1 + safe_exp(t)))\n",
    "\n",
    "    w = initial_w\n",
    "\n",
    "    for i in range(max_iters):\n",
    "        # Gradient descent\n",
    "        pred = tx @ w\n",
    "        weights = np.where(y == 1, weight, 1)\n",
    "        gradient = -tx.T @ (y * weights * stable_sigmoid(-y * pred)) / len(y)\n",
    "        w = w - gamma * gradient\n",
    "\n",
    "        # compute loss\n",
    "        train_loss = np.sum(weights * np.log(1 + safe_exp(-y * (tx @ w)))) / len(y)\n",
    "\n",
    "        # log\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Current iteration: {i}, train loss: {train_loss}\")\n",
    "\n",
    "    print(\"Done\\n\")\n",
    "\n",
    "    return w, train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    is_fix_nan: bool,\n",
    "    is_fix_outliers: bool,\n",
    "    is_normalize: bool,\n",
    "    is_remove_corr: bool,\n",
    "    is_weight: bool,\n",
    "):\n",
    "    # 1. fixing nan\n",
    "    if is_fix_nan:\n",
    "        x_train = np.nan_to_num(x_train)\n",
    "        x_val = np.nan_to_num(x_val)\n",
    "\n",
    "    # 2. removing outliers\n",
    "    if is_fix_outliers:\n",
    "        quant = np.quantile(x_train, 0.9, axis=0)\n",
    "        x_train = np.minimum(x_train, quant)\n",
    "        x_val = np.minimum(x_val, quant)\n",
    "\n",
    "    # 3. normalizing dataset\n",
    "    if is_normalize:\n",
    "        x_val -= np.min(x_train, axis=0)\n",
    "        x_train -= np.min(x_train, axis=0)\n",
    "        quant = np.max(x_train, axis=0)\n",
    "        x_train /= quant + 0.01\n",
    "        x_val /= quant + 0.01\n",
    "\n",
    "    # 4. removing correlated features\n",
    "    if is_remove_corr:\n",
    "        arr = np.corrcoef(x_train, rowvar=False)\n",
    "        bad = []\n",
    "        good = []\n",
    "        for i in range(arr.shape[0]):\n",
    "            ok = True\n",
    "            for j in range(i):\n",
    "                if i != j and abs(arr[i][j]) > 0.8:\n",
    "                    ok = False\n",
    "            if ok:\n",
    "                good.append(i)\n",
    "            else:\n",
    "                bad.append(i)\n",
    "        x_train = x_train[:, good]\n",
    "        x_val = x_val[:, good]\n",
    "\n",
    "    # 5. setting weight for cross entropy\n",
    "    if is_weight:\n",
    "        weight = 10\n",
    "    else:\n",
    "        weight = 1\n",
    "\n",
    "    # logistic regression\n",
    "    w, _ = logistic_regression(\n",
    "        y_train, x_train, np.zeros(x_train.shape[1]), 100, 0.1, weight\n",
    "    )\n",
    "    y_val_pred = 2 * (x_val @ w > 0) - 1\n",
    "    print(\"Accuracy\", np.mean(y_val_pred == y_val))\n",
    "\n",
    "    # f1-score calculation\n",
    "    ind = y_val == 1\n",
    "    true_pos = sum(y_val[ind] == y_val_pred[ind])\n",
    "    false_neg = sum(y_val[ind] != y_val_pred[ind])\n",
    "\n",
    "    ind = y_val == -1\n",
    "    # true_neg = sum(y_val[ind] == y_val_pred[ind])\n",
    "    false_pos = sum(y_val[ind] != y_val_pred[ind])\n",
    "\n",
    "    pr = true_pos / (true_pos + false_pos)\n",
    "    rec = true_pos / (true_pos + false_neg)\n",
    "    f1 = 2 * pr * rec / (pr + rec)\n",
    "\n",
    "    return f1, pr, rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All of the data preprocessing methods\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kelu/code/epfl/com433_ml/epfl-ml2024-proj-1/.venv/lib/python3.10/site-packages/numpy/lib/_function_base_impl.py:2999: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "/Users/kelu/code/epfl/com433_ml/epfl-ml2024-proj-1/.venv/lib/python3.10/site-packages/numpy/lib/_function_base_impl.py:3000: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration: 0, train loss: 1.222408117238126\n",
      "Current iteration: 10, train loss: 1.0802942149030645\n",
      "Current iteration: 20, train loss: 1.0089608618312393\n",
      "Current iteration: 30, train loss: 0.9686065271638179\n",
      "Current iteration: 40, train loss: 0.9434353572903723\n",
      "Current iteration: 50, train loss: 0.9265116495519763\n",
      "Current iteration: 60, train loss: 0.9144590818552418\n",
      "Current iteration: 70, train loss: 0.905482360782001\n",
      "Current iteration: 80, train loss: 0.8985550498147118\n",
      "Current iteration: 90, train loss: 0.8930542253511482\n",
      "Done\n",
      "\n",
      "Accuracy 0.7504418845614677\n",
      "Precision: 0.2332287766872407\n",
      "Recall: 0.7904663923182441\n",
      "F1-Score: 0.3601843894054223\n",
      "----------------------------------------------------\n",
      "Without Missing Value Replacement\n",
      "\n",
      "Current iteration: 0, train loss: nan\n",
      "Current iteration: 10, train loss: nan\n",
      "Current iteration: 20, train loss: nan\n",
      "Current iteration: 30, train loss: nan\n",
      "Current iteration: 40, train loss: nan\n",
      "Current iteration: 50, train loss: nan\n",
      "Current iteration: 60, train loss: nan\n",
      "Current iteration: 70, train loss: nan\n",
      "Current iteration: 80, train loss: nan\n",
      "Current iteration: 90, train loss: nan\n",
      "Done\n",
      "\n",
      "Accuracy 0.9111354909489852\n",
      "Precision: nan\n",
      "Recall: 0.0\n",
      "F1-Score: nan\n",
      "----------------------------------------------------\n",
      "without Outliers Clipping\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qs/8z9xcm210_n3npgpj0yx6_k00000gn/T/ipykernel_43332/2107395344.py:70: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  pr = true_pos / (true_pos + false_pos)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration: 0, train loss: 1.2358608689215391\n",
      "Current iteration: 10, train loss: 1.1722389572471885\n",
      "Current iteration: 20, train loss: 1.1269543551894587\n",
      "Current iteration: 30, train loss: 1.093924200683148\n",
      "Current iteration: 40, train loss: 1.06919972428536\n",
      "Current iteration: 50, train loss: 1.0502154278096971\n",
      "Current iteration: 60, train loss: 1.0352863467166007\n",
      "Current iteration: 70, train loss: 1.0232862938801757\n",
      "Current iteration: 80, train loss: 1.013447313473999\n",
      "Current iteration: 90, train loss: 1.005234881429655\n",
      "Done\n",
      "\n",
      "Accuracy 0.6962881696836716\n",
      "Precision: 0.19401041666666666\n",
      "Recall: 0.7664609053497943\n",
      "F1-Score: 0.3096425602660017\n",
      "----------------------------------------------------\n",
      "Without Normalization\n",
      "\n",
      "Current iteration: 0, train loss: 88.23957659631384\n",
      "Current iteration: 10, train loss: 88.23957659631384\n",
      "Current iteration: 20, train loss: 88.23957659631384\n",
      "Current iteration: 30, train loss: 88.23957659631384\n",
      "Current iteration: 40, train loss: 88.23957659631384\n",
      "Current iteration: 50, train loss: 88.23957659631384\n",
      "Current iteration: 60, train loss: 88.23957659631384\n",
      "Current iteration: 70, train loss: 91.17604234036861\n",
      "Current iteration: 80, train loss: 91.17604234036861\n",
      "Current iteration: 90, train loss: 91.17604234036861\n",
      "Done\n",
      "\n",
      "Accuracy 0.9111354909489852\n",
      "Precision: nan\n",
      "Recall: 0.0\n",
      "F1-Score: nan\n",
      "----------------------------------------------------\n",
      "Without Correlated Features Removal\n",
      "\n",
      "Current iteration: 0, train loss: 1.1977582837497325\n",
      "Current iteration: 10, train loss: 1.5543156361078732\n",
      "Current iteration: 20, train loss: 1.1810494071216702\n",
      "Current iteration: 30, train loss: 0.9960156802584014\n",
      "Current iteration: 40, train loss: 0.927589441769324\n",
      "Current iteration: 50, train loss: 0.9019559035642831\n",
      "Current iteration: 60, train loss: 0.8915926129212904\n",
      "Current iteration: 70, train loss: 0.8860861433150914\n",
      "Current iteration: 80, train loss: 0.8821138596375842\n",
      "Current iteration: 90, train loss: 0.8788498159281173\n",
      "Done\n",
      "\n",
      "Accuracy 0.7462058877308466\n",
      "Precision: 0.23063906032251644\n",
      "Recall: 0.7945816186556928\n",
      "F1-Score: 0.3575065576299954\n",
      "----------------------------------------------------\n",
      "Without Weighting\n",
      "\n",
      "Current iteration: 0, train loss: 0.316438417665259\n",
      "Current iteration: 10, train loss: 0.2890680435578668\n",
      "Current iteration: 20, train loss: 0.2834056065292384\n",
      "Current iteration: 30, train loss: 0.27841567729343525\n",
      "Current iteration: 40, train loss: 0.2740078569740982\n",
      "Current iteration: 50, train loss: 0.2701059056931412\n",
      "Current iteration: 60, train loss: 0.26664344652884164\n",
      "Current iteration: 70, train loss: 0.2635629517713256\n",
      "Current iteration: 80, train loss: 0.2608147227831981\n",
      "Current iteration: 90, train loss: 0.2583559154681466\n",
      "Done\n",
      "\n",
      "Accuracy 0.9111354909489852\n",
      "Precision: nan\n",
      "Recall: 0.0\n",
      "F1-Score: nan\n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"All of the data preprocessing methods\\n\")\n",
    "f1, pr, rec = experiment(x_train, y_train, x_val, y_val, 1, 1, 1, 1, 1)\n",
    "print(f\"Precision: {pr}\")\n",
    "print(f\"Recall: {rec}\")\n",
    "print(f\"F1-Score: {f1}\")\n",
    "print(\"----------------------------------------------------\")\n",
    "\n",
    "print(\"Without Missing Value Replacement\\n\")\n",
    "f1, pr, rec = experiment(x_train, y_train, x_val, y_val, 0, 1, 1, 1, 1)\n",
    "print(f\"Precision: {pr}\")\n",
    "print(f\"Recall: {rec}\")\n",
    "print(f\"F1-Score: {f1}\")\n",
    "print(\"----------------------------------------------------\")\n",
    "\n",
    "print(\"without Outliers Clipping\\n\")\n",
    "f1, pr, rec = experiment(x_train, y_train, x_val, y_val, 1, 0, 1, 1, 1)\n",
    "print(f\"Precision: {pr}\")\n",
    "print(f\"Recall: {rec}\")\n",
    "print(f\"F1-Score: {f1}\")\n",
    "print(\"----------------------------------------------------\")\n",
    "\n",
    "print(\"Without Normalization\\n\")\n",
    "f1, pr, rec = experiment(x_train, y_train, x_val, y_val, 1, 1, 0, 1, 1)\n",
    "print(f\"Precision: {pr}\")\n",
    "print(f\"Recall: {rec}\")\n",
    "print(f\"F1-Score: {f1}\")\n",
    "print(\"----------------------------------------------------\")\n",
    "\n",
    "print(\"Without Correlated Features Removal\\n\")\n",
    "f1, pr, rec = experiment(x_train, y_train, x_val, y_val, 1, 1, 1, 0, 1)\n",
    "print(f\"Precision: {pr}\")\n",
    "print(f\"Recall: {rec}\")\n",
    "print(f\"F1-Score: {f1}\")\n",
    "print(\"----------------------------------------------------\")\n",
    "\n",
    "print(\"Without Weighting\\n\")\n",
    "f1, pr, rec = experiment(x_train, y_train, x_val, y_val, 1, 1, 1, 1, 0)\n",
    "print(f\"Precision: {pr}\")\n",
    "print(f\"Recall: {rec}\")\n",
    "print(f\"F1-Score: {f1}\")\n",
    "print(\"----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
